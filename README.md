# Reddit Evidence Usage and Stance Analysis: Elon Musk Discussions

## Overview

This repository contains the code and analysis for a project investigating how evidence indicators are used and interpreted within online discussions on Reddit, using the polarizing figure Elon Musk as a case study. The project explores the relationship between citing external sources (indicated by URLs and keywords), user sentiment (measured by VADER), user stance (classified by OpenAI's GPT), and engagement patterns within different subreddit communities (`r/technology` and `r/EnoughMuskSpam`).

The core idea is that evidence alone doesn't guarantee persuasion online; its reception is heavily influenced by pre-existing beliefs and community norms. This project uses computational methods to scrape, analyze, and prepare data for visualizing these dynamics.

**--> For the complete methodology, code implementation, detailed analysis, and findings, please refer to the Jupyter Notebook: [`Computational_Notebook.ipynb`](Computational_Notebook.ipynb) <--**


![Screenshot 2025-04-07 203310](https://github.com/user-attachments/assets/8f350117-c418-442c-b9cc-99c96ab5b623)

## Research Focus

This analysis aims to address questions such as:

1.  How often do users in different subreddits include evidence indicators (links/keywords) when discussing Elon Musk?
2.  Is there a correlation between using evidence indicators and engagement metrics (like comment scores)?
3.  Can we identify instances where the same evidence source is potentially used to support conflicting stances?
4.  How do these patterns differ between subreddits with varying prevailing sentiments towards the topic (`r/technology` vs. `r/EnoughMuskSpam`)?

## Methodology

The project employed the following computational steps, detailed within the notebook:

*   **Data Collection:** Scraped posts and comments related to "Elon Musk" from `r/technology` and `r/EnoughMuskSpam` using the Reddit API via the **PRAW** library.
*   **Evidence Indicator Identification:** Parsed post/comment text to identify the presence of URLs and predefined keywords often associated with citing evidence.
*   **Sentiment Analysis:** Applied the **VADER** sentiment analysis tool to calculate a sentiment polarity score (`compound`) for each post and comment.
*   **Stance Detection:** Utilized **OpenAI's GPT API** (specifically `gpt-3.5-turbo`) to classify the author's stance towards Elon Musk ("Pro-Musk", "Anti-Musk", "Neutral/Mixed", "Unclear") for each post/comment.
*   **Network Data Preparation:** Generated node lists (authors with aggregated attributes like average sentiment, dominant stance, evidence usage count) and edge lists (reply interactions) formatted for import into the **Gephi** network analysis software.

## Data

*   **Source:** Reddit API (data collected around [Specify Month/Year if desired, e.g., April 2024]).
*   **Generated Files:** The Jupyter notebook generates several CSV files upon successful execution, including:
    *   `reddit_data_Elon_Musk_..._final_analysis.csv`: The main dataset containing all posts/comments with added columns for evidence indicators, VADER scores, and OpenAI stance.
    *   `gephi_nodes_Elon_Musk.csv`: Node list for Gephi (authors and basic attributes).
    *   `gephi_edges_Elon_Musk.csv`: Edge list for Gephi (reply interactions).
    *   `gephi_node_attributes_extra_Elon_Musk.csv`: Additional aggregated node attributes (average sentiment, dominant stance) for merging in Gephi.
    *(Note: These data files are typically generated by running the notebook and may not be included directly in the repository by default).*

## Tools Used

*   **Python 3**
*   **Jupyter Notebook**
*   **PRAW:** Python Reddit API Wrapper
*   **Pandas:** Data manipulation and analysis
*   **VADER Sentiment:** Rule-based sentiment analysis
*   **OpenAI Python Library:** Interacting with GPT API for stance detection
*   **Gephi:** Network analysis and visualization software

## Repository Structure.
├── Computational_Notebook.ipynb # Main Jupyter Notebook with all code and explanations
├── README.md # This file

## Getting Started / How to Use

1.  **Clone the Repository:**
    ```bash
    git clone https://github.com/YourUsername/YourRepositoryName.git
    cd YourRepositoryName
    ```
2.  **Set up Python Environment:** (Recommended)
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```
3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(If `requirements.txt` is not provided, manually install: `pip install praw pandas vaderSentiment openai scipy`)*
4.  **Configure API Keys:**
    *   **Reddit API:** You need a Reddit account and must create a script app [here](https://www.reddit.com/prefs/apps) to get a `Client ID` and `Client Secret`.
    *   **OpenAI API:** You need an OpenAI account and an API key from [their platform](https://platform.openai.com/api-keys).
    *   **Set Environment Variables:** It is strongly recommended to set your API keys as environment variables rather than hardcoding them. The notebook code attempts to read:
        *   `REDDIT_CLIENT_ID`
        *   `REDDIT_CLIENT_SECRET`
        *   `REDDIT_USER_AGENT` (Set this to something descriptive, e.g., "MuskEvidenceAnalysis by u/YourRedditUsername")
        *   `OPENAI_API_KEY`
5.  **Run the Notebook:** Launch Jupyter Notebook (`jupyter notebook`) and open `Computational_Notebook.ipynb`. Execute the cells sequentially.
6.  **Visualize in Gephi:** Import the generated `gephi_nodes...csv`, `gephi_edges...csv`, and `gephi_node_attributes_extra...csv` files into Gephi following the import/merge procedures detailed in the notebook or Gephi documentation.

## Important Notes & Disclaimer

*   **API Keys:** Keep your API keys secure. Do not commit them directly into the repository. Use environment variables or other secure configuration methods.
*   **OpenAI Costs:** Using the OpenAI API incurs costs based on token usage. Be mindful of the amount of data being processed.
*   **Reddit API Limits:** Be respectful of Reddit's API rate limits. The code includes `time.sleep()` calls, but excessive scraping can lead to temporary blocks. Adhere to Reddit's API Terms of Service.
*   **Data Snapshot:** The analysis reflects the state of discussion during the specific data collection period. Online conversations evolve rapidly.
*   **Ethical Considerations:** Usernames are included in the analysis. While public data, consider the implications of analyzing online discussions. The analysis focuses on content patterns, not individual profiling.

